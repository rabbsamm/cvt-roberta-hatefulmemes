{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2482bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel, AutoModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from string import digits\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.functional.classification import auroc, accuracy\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import lightning.pytorch as lp\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accd525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\rabby\\CS 7643 - Deep Learning\\Project')\n",
    "\n",
    "config = {\n",
    "    'model_name': 'roberta-base',\n",
    "    'n_labels': 2,\n",
    "    'batch_size': 64,\n",
    "    'dropout': 0.2,\n",
    "    'lr': 1e-5,\n",
    "    'n_epochs': 5,\n",
    "    'device': 'cuda',\n",
    "    'n_threads': 100,\n",
    "    'reload': True,\n",
    "    'text_rl': r'C:\\Users\\rabby\\CS 7643 - Deep Learning\\Project\\Data\\pol_0616-1119_labeled\\pol_train_10k.csv',\n",
    "    'ckpt_path': os.getcwd(),\n",
    "    'val_every': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8169b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_path = r'Data\\pol_0616-1119_labeled\\pol_062016-112019_labeled.ndjson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccac0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_Post(post):\n",
    "    # Remove HTML tags\n",
    "    cleaned_post = re.sub('<.*?>', '', post)\n",
    "\n",
    "    # Unescape HTML entities\n",
    "    cleaned_post = unescape(cleaned_post)\n",
    "\n",
    "    # Remove line breaks and extra spaces\n",
    "    cleaned_post = cleaned_post.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "    \n",
    "    x = cleaned_post.split('>')\n",
    "    cleaned_post = ' '.join(x).strip()\n",
    "    cleaned_post = cleaned_post.lstrip(digits)\n",
    "\n",
    "    return cleaned_post\n",
    "\n",
    "def Load_Pol(pol_path, n_threads = 10**5, test_size = 0.2, random_state = 24):\n",
    "    \n",
    "    #Load first n_threads rows from 4chan data\n",
    "    pol_raw = pd.read_json(pol_path, lines = True, nrows = n_threads)\n",
    "    #Declare variables for storage of posts and toxicity scores\n",
    "    posts = []\n",
    "    toxicity = []\n",
    "    inflammatory = []\n",
    "    #Extract posts, scores from nested dictionaries\n",
    "    for i in range(n_threads):\n",
    "        thread = pol_raw.loc[i][0]\n",
    "        n_posts = len(thread)\n",
    "        for j in range(n_posts):\n",
    "            try:\n",
    "                posts.append(thread[j]['com'])\n",
    "                toxicity.append(thread[j]['perspectives']['SEVERE_TOXICITY'])\n",
    "                inflammatory.append(thread[j]['perspectives']['INFLAMMATORY'])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    #Create pol_data df\n",
    "    pol_data = pd.DataFrame(data = {'Posts': posts, 'Toxicity': toxicity, 'Inflammatory': inflammatory})\n",
    "    #Clean comments\n",
    "    pol_data.loc[:, 'Posts'] = pol_data.loc[:, 'Posts'].apply(Clean_Post)\n",
    "    #Set Toxic Flag to 1 for posts exceeding toxicity and inflammatory thresholds\n",
    "\n",
    "    #Split Train and Val\n",
    "    pol_train, pol_val = train_test_split(pol_data, test_size = test_size, random_state = random_state)\n",
    "    \n",
    "    return pol_train, pol_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c3adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pol_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length = 64):\n",
    "        #Declare variables\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Select index\n",
    "        item = self.data.iloc[index]\n",
    "        #Extract posts and toxicity flag\n",
    "        post = str(item.Posts)\n",
    "        label = torch.Tensor([item.loc[['Toxicity', 'Inflammatory']].tolist()])\n",
    "        #Convert to tokens\n",
    "        tokens = self.tokenizer.encode_plus(post, add_special_tokens = True, return_tensors = 'pt', truncation = True, \n",
    "                                           max_length = self.max_length, padding = 'max_length', return_attention_mask = True)\n",
    "        \n",
    "        return {'input_ids': tokens.input_ids.flatten(), 'attention_mask': tokens.attention_mask.flatten(), 'labels': label}\n",
    "    \n",
    "class Pol_Data_Module(lp.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, train, val, test = None, batch_size = 16, max_length = 64,  model = 'roberta-base'):\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        if test == None:\n",
    "            self.test = val\n",
    "        else:\n",
    "            self.test = test\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.model = model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model)\n",
    "        \n",
    "    def setup(self, stage = None):\n",
    "        self.train_ds = Pol_Dataset(self.train, self.tokenizer, max_length = self.max_length)\n",
    "        self.val_ds = Pol_Dataset(self.val, self.tokenizer, max_length = self.max_length)\n",
    "        self.test_ds = Pol_Dataset(self.test, self.tokenizer, max_length = self.max_length)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size = self.batch_size, shuffle = True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size = self.batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e51f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roberta_Pol(lp.LightningModule):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, self.config['n_labels'])\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.tloss = []\n",
    "        self.vloss = []\n",
    "           \n",
    "    def forward(self, input_ids, attention_mask, labels = None):\n",
    "        out = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        out = torch.mean(out.last_hidden_state, 1)\n",
    "        # final logits\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        loss, out, y = self._common_step(batch, batch_index)\n",
    "        self.training_step_outputs.append(loss)\n",
    "        self.log(\"Training Loss\", loss, prog_bar = True, logger = True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_mean = torch.stack(self.training_step_outputs).mean()\n",
    "        self.tloss.append(float(epoch_mean.detach().cpu().numpy()))\n",
    "        self.training_step_outputs.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_index):\n",
    "        loss, out, y = self._common_step(batch, batch_index)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        self.log(\"Validation Loss\", loss, prog_bar = True, logger = True)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_mean = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.vloss.append(float(epoch_mean.detach().cpu().numpy()))\n",
    "        self.validation_step_outputs.clear()\n",
    "    \n",
    "    def test_step(self, batch, batch_index):\n",
    "        loss, out, y = self._common_step(batch, batch_index)\n",
    "        self.log(\"Test Loss\", loss, prog_bar = True, logger = True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_index):\n",
    "        loss, out, y = self._common_step(batch, batch_index)\n",
    "        return loss\n",
    "    \n",
    "    def _common_step(self, batch, batch_index):\n",
    "        x = batch['input_ids']\n",
    "        y = batch['labels'].squeeze(1)\n",
    "        attn_mask = batch['attention_mask']\n",
    "        out = self.forward(x, attn_mask)\n",
    "        y = y.to(config['device'])\n",
    "        loss = self.loss(out, y)\n",
    "        return loss, out, y\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr = self.config['lr'])\n",
    "        return [optimizer]\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        self.vloss.pop()\n",
    "        plt.plot(self.tloss, label = 'Training')\n",
    "        plt.plot(self.vloss, label = 'Validation')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_auroc(self):\n",
    "        self.vauroc.pop()\n",
    "        plt.plot(self.tauroc, label = 'Training')\n",
    "        plt.plot(self.vauroc, label = 'Validation')\n",
    "        plt.title('AUROC')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_accuracy(self):\n",
    "        self.vacc.pop()\n",
    "        plt.plot(self.tacc, label = 'Training')\n",
    "        plt.plot(self.vacc, label = 'Validation')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144af2ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "ModelCheckpoint(save_top_k=2, monitor=None) is not a valid configuration. No quantity for top_k to track.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m      2\u001b[0m         dirpath \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpt_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      3\u001b[0m         filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinetuned_Roberta_CKPT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m         every_n_epochs \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_every\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m         save_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      6\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:253\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, dirpath, filename, monitor, verbose, save_last, save_top_k, save_weights_only, mode, auto_insert_metric_name, every_n_train_steps, train_time_interval, every_n_epochs, save_on_train_epoch_end, enable_version_counter)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_ckpt_dir(dirpath, filename)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_triggers(every_n_train_steps, every_n_epochs, train_time_interval)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__validate_init_configuration()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:452\u001b[0m, in \u001b[0;36mModelCheckpoint.__validate_init_configuration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombination of parameters every_n_train_steps=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_every_n_train_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevery_n_epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_every_n_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and train_time_interval=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_time_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be mutually exclusive.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     )\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_top_k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;66;03m# -1: save all epochs, 0: nothing is saved, 1: save last epoch\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelCheckpoint(save_top_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_top_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, monitor=None) is not a valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m configuration. No quantity for top_k to track.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m     )\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: ModelCheckpoint(save_top_k=2, monitor=None) is not a valid configuration. No quantity for top_k to track."
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath = config['ckpt_path'], \n",
    "        filename = \"Finetuned_Roberta_CKPT\",\n",
    "        every_n_epochs = config['val_every'],\n",
    "        save_top_k = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datamodule\n",
    "if config['reload']:\n",
    "    pol_train, pol_val = Load_Pol(pol_path, n_threads = config['n_threads'])\n",
    "    pol_data_module = Pol_Data_Module(pol_train, pol_val, batch_size = config['batch_size'])\n",
    "else:\n",
    "    pol_train, pol_val = train_test_split(pd.read_csv(config['text_rl']), test_size = 0.2, random_state = 24)\n",
    "    pol_data_module = Pol_Data_Module(pol_train, pol_val, batch_size = config['batch_size'])\n",
    "\n",
    "# model\n",
    "model = Roberta_Pol(config)\n",
    "\n",
    "# trainer and fit\n",
    "trainer = lp.Trainer(max_epochs = config['n_epochs'], devices = 1, accelerator = \"gpu\", num_sanity_val_steps = 0,\n",
    "                    callbacks = [checkpoint_callback], default_root_dir = config['ckpt_path'], \n",
    "                     check_val_every_n_epoch = config['val_every'])\n",
    "trainer.fit(model, pol_data_module)\n",
    "trainer.validate(model, pol_data_module)\n",
    "trainer.test(model, pol_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385661aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, r\"Unimodal Models\\Finetuned_Roberta.pth.tar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
